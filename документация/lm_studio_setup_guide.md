# Руководство по настройке LM Studio для локального ИИ

## Обзор

Это руководство содержит пошаговые инструкции по установке и настройке LM Studio для использования локальных LLM моделей в системе расчета коэффициентов усушки.

## Установка LM Studio

### 1. Скачивание LM Studio
1. Перейдите на официальный сайт: https://lmstudio.ai/
2. Нажмите кнопку "Download" для вашей операционной системы
3. Дождитесь завершения загрузки установочного файла

### 2. Установка приложения
1. Запустите загруженный установочный файл
2. Следуйте инструкциям установщика
3. После установки запустите LM Studio

## Загрузка модели

### 1. Открытие LM Studio
1. Запустите приложение LM Studio
2. Дождитесь загрузки интерфейса

### 2. Поиск и загрузка модели
1. Перейдите во вкладку "Local Inference"
2. Нажмите кнопку "Download" или "+"
3. В строке поиска введите название модели, например:
   - "Qwen3" для русскоязычной модели
   - "Llama 3" для универсальной модели
   - "Gemma3" для модели от Google
4. Выберите подходящую модель из списка результатов
5. Нажмите "Download" для загрузки модели

### 3. Рекомендуемые модели для системы
- **Qwen3 8B** - отличная поддержка русского языка
- **Gemma3 8B** - хорошая поддержка русского языка
- **Llama 3 8B** - универсальная модель
- **Vikhr-Qwen-2.5-0.5B-instruct** - специализированная русская модель

## Запуск сервера

### 1. Переход во вкладку сервера
1. В LM Studio перейдите во вкладку "Server"
2. Убедитесь, что нужная модель выбрана в выпадающем списке

### 2. Настройка параметров сервера
1. Проверьте порт сервера (по умолчанию 1234)
2. При необходимости измените порт в настройках
3. Убедитесь, что выбрана правильная модель

### 3. Запуск сервера
1. Нажмите кнопку "Start Server"
2. Дождитесь появления надписи "Server running"
3. Запомните URL сервера (по умолчанию http://localhost:1234/v1)

## Настройка в системе расчета усушки

### 1. Открытие настроек ИИ-чата
1. Запустите систему расчета усушки
2. Перейдите в меню "Настройки" -> "Настройки ИИ-чата"

### 2. Включение локального ИИ
1. Установите флажок "Включить локальный ИИ"
2. В поле "URL сервера LM Studio" введите URL сервера (по умолчанию http://localhost:1234/v1)
3. В поле "Модель локального ИИ" введите название модели (например, "gpt-3.5-turbo" для совместимости)

### 3. Сохранение настроек
1. Нажмите кнопку "Сохранить"
2. Перезапустите систему при необходимости

## Проверка работы

### 1. Тестирование подключения
1. Перейдите во вкладку "ИИ-чат"
2. Введите тестовый вопрос, например: "Как работает система расчета коэффициентов усушки?"
3. Нажмите "Отправить"
4. Дождитесь ответа от локального ИИ

### 2. Проверка логов
1. Проверьте логи системы на наличие ошибок подключения
2. Убедитесь, что запросы успешно отправляются и получаются

## Оптимизация производительности

### 1. Настройки контекста
1. В LM Studio можно настроить размер контекста модели
2. Для системы расчета усушки рекомендуется использовать контекст 4096-8192 токенов

### 2. Параметры генерации
1. Температура: 0.7 (баланс между креативностью и точностью)
2. Top-p: 0.9 (разнообразие ответов)
3. Максимальное количество токенов: 500-1000 (достаточно для большинства ответов)

### 3. Использование GPU
1. При наличии совместимой видеокарты NVIDIA активируйте GPU ускорение
2. В LM Studio перейдите в настройки и включите использование GPU
3. Убедитесь, что установлены последние драйверы NVIDIA

## Устранение неполадок

### 1. Проблемы с подключением
**Симптомы**: Ошибки подключения к серверу LM Studio
**Решения**:
- Проверьте, запущен ли сервер LM Studio
- Убедитесь, что URL сервера указан правильно
- Проверьте, не блокирует ли брандмауэр подключение

### 2. Медленная работа
**Симптомы**: Долгое время генерации ответов
**Решения**:
- Закройте другие приложения для освобождения ресурсов
- Используйте модели меньшего размера
- Активируйте GPU ускорение при наличии видеокарты

### 3. Ошибки памяти
**Симптомы**: Сбои при загрузке или работе модели
**Решения**:
- Используйте квантованные версии моделей (GGUF)
- Выберите модель меньшего размера
- Закройте другие приложения

### 4. Проблемы с русским языком
**Симптомы**: Некорректная обработка русских запросов
**Решения**:
- Используйте специализированные русскоязычные модели
- Проверьте кодировку запросов (должна быть UTF-8)
- Убедитесь, что модель поддерживает русский язык

## Полезные советы

### 1. Управление моделями
- Регулярно обновляйте модели для получения лучших результатов
- Удаляйте неиспользуемые модели для освобождения места на диске
- Создавайте резервные копии важных моделей

### 2. Мониторинг ресурсов
- Следите за использованием оперативной памяти
- Контролируйте температуру видеокарты при длительной работе
- Используйте диспетчер задач для мониторинга нагрузки

### 3. Обновление LM Studio
- Регулярно обновляйте LM Studio для получения новых функций
- Следите за изменениями в API для обеспечения совместимости

## Требования к системе

### Минимальные требования:
- **Операционная система**: Windows 10, macOS 10.15 или новее, современный Linux
- **Процессор**: Современный процессор с 4+ ядрами
- **Оперативная память**: 8 ГБ (для моделей 3B-4B)
- **Место на диске**: 4-6 ГБ свободного места для модели
- **Графический процессор**: Не обязателен, но рекомендуется для ускорения

### Рекомендуемые требования:
- **Операционная система**: Windows 11, macOS 12 или новее, современный Linux
- **Процессор**: Процессор с 8+ ядрами
- **Оперативная память**: 16-32 ГБ
- **Графический процессор**: NVIDIA GPU с 8+ ГБ VRAM
- **Место на диске**: 10-70 ГБ свободного места (в зависимости от модели)

## Заключение

LM Studio предоставляет удобный способ использования локальных LLM моделей без подключения к интернету. Правильная настройка и выбор подходящей модели позволят эффективно использовать ИИ в системе расчета коэффициентов усушки с сохранением конфиденциальности данных и без дополнительных затрат на API.