# Руководство по русскоязычным LLM моделям для локального использования

## Обзор

Это руководство содержит информацию о русскоязычных LLM моделях, которые можно использовать с локальной реализацией ИИ в системе расчета коэффициентов усушки. Все модели доступны в формате GGUF для использования с LM Studio.

## Рекомендуемые русскоязычные модели

### 1. Qwen3 (Alibaba)
- **Модели**: Qwen3 8B, Qwen3 72B
- **Особенности**: Отличная поддержка русского языка, мультимодальная модель
- **Размеры**: 8B (средний), 72B (большой)
- **Ссылка**: https://huggingface.co/Qwen/Qwen3
- **GGUF версии**: Доступны на HuggingFace

### 2. Gemma3 (Google)
- **Модели**: Gemma3 8B
- **Особенности**: Хорошая поддержка русского языка, эффективная модель
- **Размеры**: 8B (средний)
- **Ссылка**: https://huggingface.co/google/gemma-3
- **GGUF версии**: Доступны на HuggingFace

### 3. ruGPT-3.5 (SberDevices)
- **Модели**: ruGPT-3.5 13B
- **Особенности**: Специализированная русскоязычная модель
- **Размеры**: 13B (большой)
- **Ссылка**: https://huggingface.co/sberdevices/rugpt35
- **GGUF версии**: Доступны на HuggingFace

### 4. Siberian Mouse (DeepPavlov)
- **Модели**: Siberian Mouse 7B
- **Особенности**: Русскоязычная модель от DeepPavlov
- **Размеры**: 7B (средний)
- **Ссылка**: https://huggingface.co/DeepPavlov/siberian_mouse
- **GGUF версии**: Доступны на HuggingFace

### 5. Vikhr-Qwen-2.5-0.5B-instruct
- **Модели**: Vikhr-Qwen-2.5-0.5B-instruct
- **Особенности**: Специализированная русская модель, обученная на русскоязычном датасете
- **Размеры**: 0.5B (очень маленькая)
- **Ссылка**: https://huggingface.co/Vikhrmodels/Vikhr-Qwen-2.5-0.5B-instruct-GGUF
- **GGUF версии**: Доступны на HuggingFace

## Рекомендации по выбору моделей в зависимости от аппаратных характеристик

### Для компьютеров с ограниченными ресурсами (8 ГБ RAM, без GPU):
- **Рекомендуемые модели**: Qwen3 4B, Gemma3 4B, Vikhr-Qwen-2.5-0.5B-instruct
- **Особенности**: Эти модели могут работать в режиме CPU inference
- **Производительность**: Умеренная скорость, приемлемое качество

### Для средних конфигураций (16 ГБ RAM, GPU с 8 ГБ VRAM):
- **Рекомендуемые модели**: Qwen3 8B, Gemma3 8B, Llama 3 8B
- **Особенности**: Эффективно работают как на CPU, так и на GPU
- **Производительность**: Хорошая скорость, высокое качество

### Для мощных конфигураций (32+ ГБ RAM, GPU с 16+ ГБ VRAM):
- **Рекомендуемые модели**: Qwen3 72B, Llama 3 70B, Mixtral 8x7B
- **Особенности**: Наилучшее качество, но требуют значительных ресурсов
- **Производительность**: Высокая скорость, отличное качество

## Оптимизация производительности

### 1. Использование квантованных версий моделей (GGUF формат)
- **Преимущества**: Уменьшение требований к памяти
- **Рекомендации**: Использовать Q4_K_M или Q5_K_M квантование для баланса между качеством и производительностью

### 2. Управление ресурсами
- **Закрытие других приложений**: Освобождение ресурсов перед запуском модели
- **Использование GPU ускорения**: При наличии совместимой видеокарты
- **Выбор моделей**: Использование моделей с меньшим количеством параметров при достаточном качестве

### 3. Настройки LM Studio
- **Количество потоков CPU**: Настройка в зависимости от количества ядер процессора
- **Размер контекста**: Оптимизация под задачи системы
- **Параметры генерации**: Температура, top-p и другие параметры для баланса между креативностью и точностью

## Установка и использование моделей

### 1. Загрузка моделей
- **Через LM Studio**: Использование встроенного браузера моделей
- **Через HuggingFace**: Загрузка GGUF файлов напрямую
- **Через командную строку**: Использование git-lfs для загрузки больших файлов

### 2. Запуск моделей
- **Выбор модели в LM Studio**: Загрузка и выбор модели в интерфейсе
- **Запуск сервера**: Активация API сервера для интеграции с системой
- **Проверка доступности**: Тестирование подключения через систему

### 3. Настройка в системе
- **URL сервера**: По умолчанию http://localhost:1234/v1
- **Выбор модели**: Указание имени модели в настройках
- **Проверка работы**: Тестирование через ИИ-чат в интерфейсе

## Тестирование моделей

### 1. Качество работы с русским языком
- **Проверка грамматики**: Корректность русской грамматики
- **Понимание контекста**: Понимание специфики системы расчета усушки
- **Терминология**: Правильное использование терминов системы

### 2. Производительность
- **Время отклика**: Измерение времени генерации ответов
- **Использование ресурсов**: Мониторинг CPU/GPU и памяти
- **Стабильность**: Проверка на длительных сессиях

### 3. Совместимость
- **API совместимость**: Проверка совместимости с OpenAI API
- **Формат ответов**: Корректность формата JSON ответов
- **Обработка ошибок**: Правильная обработка ошибок подключения

## Рекомендации по использованию

### 1. Для общих задач и вопросов
- **Рекомендуемые модели**: Qwen3 8B, Gemma3 8B
- **Причины**: Хорошее соотношение качества и производительности

### 2. Для задач программирования
- **Рекомендуемые модели**: Code Llama 7B/13B, DeepSeek Coder 7B
- **Причины**: Специализация на задачах программирования

### 3. Для задач на русском языке
- **Рекомендуемые модели**: Qwen3 8B/72B, ruGPT-3.5, Siberian Mouse
- **Причины**: Отличная поддержка русского языка

## Требования к аппаратному обеспечению

### Минимальные требования:
- **CPU**: Современный процессор с 4+ ядрами
- **RAM**: 8 ГБ (для моделей 3B-4B)
- **Диск**: 4-6 ГБ свободного места для модели

### Рекомендуемые требования:
- **CPU**: Процессор с 8+ ядрами
- **RAM**: 16-32 ГБ
- **GPU**: NVIDIA GPU с 8+ ГБ VRAM (опционально, но значительно ускоряет работу)
- **Диск**: 10-70 ГБ свободного места (в зависимости от модели)

## Полезные ресурсы

### 1. Репозитории моделей
- **HuggingFace**: https://huggingface.co/models
- **LM Studio Model Hub**: Встроенный в LM Studio

### 2. Документация
- **LM Studio**: https://lmstudio.ai/docs
- **GGUF формат**: https://github.com/ggerganov/llama.cpp

### 3. Сообщества
- **LocalLLM subreddit**: https://www.reddit.com/r/LocalLLM/
- **HuggingFace Community**: https://discuss.huggingface.co/

## Заключение

Русскоязычные LLM модели открывают новые возможности для локального использования ИИ в системе расчета коэффициентов усушки. Правильный выбор модели в зависимости от аппаратных характеристик и задач позволит получить высокое качество работы без подключения к интернету и дополнительных затрат на API.